THUAT TOAN: Augmented Lagrangian Method (ALM)
DO PHUC TAP: O(k * m * n) voi k la so vong lap ngoai, m la so vong lap trong, n la chieu cua bien

PSEUDO CODE:
function augmentedLagrangian(x0, lambda0, rho0):
    // Giai bai toan: min f(x) subject to h(x) = 0
    
    x = x0
    lambda = lambda0
    rho = rho0
    
    maxOuter = 50
    epsilon = 1e-6
    rhoMax = 1e6
    
    for k = 0 to maxOuter:
        // Giai bai toan con: min L(x, lambda, rho)
        x = solveSubproblem(x, lambda, rho)
        
        // Tinh rang buoc
        h_val = h(x)
        
        // Kiem tra hoi tu
        if |h_val| < epsilon:
            break
        
        // Cap nhat Lagrange multiplier
        lambda = lambda - rho * h_val
        
        // Tang penalty parameter neu can
        if |h_val| > 0.25 * |h_prev|:
            rho = min(rho * 10, rhoMax)
        
        h_prev = h_val
    
    return x


function solveSubproblem(x, lambda, rho):
    // Giai: min L_rho(x) = f(x) - lambda*h(x) + 0.5*rho*h(x)^2
    // Su dung gradient descent voi line search
    
    maxIter = 5000
    tol = 1e-8
    alpha = 0.1
    
    for i = 0 to maxIter:
        // Tinh gradient cua augmented Lagrangian
        grad = gradientL(x, lambda, rho)
        
        normGrad = ||grad||
        if normGrad < tol:
            break
        
        // Line search (backtracking)
        L_old = L(x, lambda, rho)
        alphaTemp = alpha
        
        for ls = 0 to 30:
            x_new = x - alphaTemp * grad
            L_new = L(x_new, lambda, rho)
            
            if L_new < L_old - 0.0001 * alphaTemp * normGrad^2:
                x = x_new
                alpha = min(1.0, alphaTemp * 1.1)
                break
            
            alphaTemp = alphaTemp * 0.5
    
    return x


function gradientL(x, lambda, rho):
    h_val = h(x)
    grad_f = gradient_f(x)
    grad_h = gradient_h(x)
    
    return grad_f + (rho * h_val - lambda) * grad_h

MÔ TẢ BẰNG NGÔN NGỮ TỰ NHIÊN:

Thuật toán: Augmented Lagrangian Method

Bài toán:
• Minimize f(x)
• Subject to h(x) = 0

Hàm Lagrangian tăng cường:
• L(x, λ, ρ) = f(x) - λᵀh(x) + (ρ/2)||h(x)||²

Input:
• Điểm khởi tạo x₀
• Nhân tử Lagrange ban đầu λ₀
• Tham số penalty ρ₀ > 0

Khởi tạo:
• x = x₀
• λ = λ₀ (thường = 0)
• ρ = ρ₀

Lặp vòng ngoài:
1. Giải bài toán con:
   - x = argmin L(x, λ, ρ)
   - Dùng Gradient Descent hoặc Newton

2. Tính ràng buộc: h_val = h(x)

3. Kiểm tra hội tụ:
   - Nếu |h_val| < ε: dừng

4. Cập nhật Lagrange multiplier:
   - λ = λ - ρ × h_val

5. Tăng penalty nếu cần:
   - Nếu |h_val| > 0.25 × |h_prev|:
     * ρ = min(ρ × 10, ρₘₐₓ)

Ý tưởng:
- Kết hợp Lagrange multipliers và penalty
- Vòng ngoài: cập nhật λ, ρ
- Vòng trong: tối ưu x
   - Đưa thông tin đối ngẫu (dual)
   - Hướng dẫn cách thỏa mãn ràng buộc

2. Thành phần penalty (ρ/2)||h(x)||²:
   - Phạt vi phạm ràng buộc
   - Đẩy x về vùng khả thi

3. Tăng ρ:
   - Penalty mạnh hơn
   - Ép buộc thỏa ràng buộc chặt chẽ hơn

4. Cập nhật λ:
   - λ là "giá" của ràng buộc
   - Di chuyển theo gradient của Lagrangian

So sánh với các phương pháp khác:

1. Penalty Method:
   - Chỉ có (ρ/2)||h(x)||²
   - Cần ρ rất lớn → ill-conditioned
   - ALM khắc phục nhờ λ

2. Lagrange Multipliers:
   - Chỉ có -λᵀh(x)
   - Yêu cầu giải saddle point (khó)
   - ALM ổn định hơn nhờ penalty

Kỹ thuật cải tiến:

1. Adaptive penalty:
   - Tăng ρ thông minh
   - Giảm khi gần hội tụ

2. Warm start:
   - Dùng x cũ làm xuất phát cho vòng mới
   - Tiết kiệm tính toán

3. Inexact minimization:
   - Không cần tối ưu x hoàn toàn
   - Vài vòng Gradient Descent là đủ

Ưu điểm:
- Robust, hội tụ ổn định
- Xử lý tốt ràng buộc phi tuyến
- Không cần ρ quá lớn

Nhược điểm:
- Chậm hơn unconstrained
- Nhiều tham số cần điều chỉnh

Ứng dụng:
- Bài toán tối ưu có ràng buộc
- Control theory
- Structural optimization
- Machine learning (constrained models)
- Image processing (denoising với ràng buộc)
