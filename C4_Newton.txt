THUAT TOAN: Phuong phap Newton (Newton's Method for Optimization)
DO PHUC TAP: O(k * n^3) voi k la so vong lap, n la chieu cua bien (n^3 tu viec giai he phuong trinh tuyen tinh)

PSEUDO CODE:
function newtonMethod(x0, epsilon, maxIter):
    // x0: diem khoi dau
    // epsilon: sai so hoi tu
    
    x = x0
    
    for iter = 0 to maxIter:
        // Tinh gradient (dao ham bac 1)
        grad = gradient_f(x)
        
        // Kiem tra hoi tu
        normGrad = ||grad||
        if normGrad < epsilon:
            break
        
        // Tinh Hessian (dao ham bac 2)
        H = hessian_f(x)
        
        // Giai he phuong trinh: H * p = -grad
        // p la huong di (Newton direction)
        p = solve(H, -grad)
        
        // Line search backtracking
        alpha = 1.0
        f_old = f(x)
        
        for ls = 0 to 20:
            x_new = x + alpha * p
            f_new = f(x_new)
            
            if f_new < f_old:
                x = x_new
                break
            
            alpha = alpha * 0.5
    
    return x


function solve(A, b):
    // Giai he phuong trinh tuyen tinh A*x = b
    // Su dung Gaussian elimination
    
    n = size(A)
    augmented = [A | b]
    
    // Khu Gauss (Forward elimination)
    for i = 0 to n-1:
        // Chon pivot
        maxRow = i
        for k = i+1 to n-1:
            if |augmented[k][i]| > |augmented[maxRow][i]|:
                maxRow = k
        swap(augmented[i], augmented[maxRow])
        
        // Kiem tra singular
        if |augmented[i][i]| < 1e-12:
            augmented[i][i] = 1e-12
        
        // Khu xuong
        for k = i+1 to n-1:
            factor = augmented[k][i] / augmented[i][i]
            for j = i to n:
                augmented[k][j] = augmented[k][j] - factor * augmented[i][j]
    
    // Thay nguoc (Back substitution)
    x = vector(n)
    for i = n-1 down to 0:
        x[i] = augmented[i][n]
        for j = i+1 to n-1:
            x[i] = x[i] - augmented[i][j] * x[j]
        x[i] = x[i] / augmented[i][i]
    
    return x


function hessian_f(x):
    // Tinh ma tran Hessian (dao ham bac 2)
    // H[i][j] = d^2f / (dx_i * dx_j)
    
    n = length(x)
    H = matrix(n, n)
    
    for i = 0 to n-1:
        for j = 0 to n-1:
            H[i][j] = secondDerivative(f, x, i, j)
    
    return H


// Cong thuc Newton:

MÔ TẢ BẰNG NGÔN NGỮ TỰ NHIÊN:

Thuật toán: Phương pháp Newton

Input:
• Hàm mục tiêu f(x), khả vi bậc 2
• Điểm khởi tạo x₀ ∈ ℝⁿ
• Ngưỡng hội tụ ε > 0
• Số vòng lặp tối đa kₘₐₓ

Khởi tạo:
• x = x₀
• k = 0

Lặp:
1. Tính gradient: g = ∇f(xₖ)

2. Kiểm tra hội tụ:
   - Nếu ||g|| < ε hoặc k > kₘₐₓ: dừng

3. Tính Hessian: H = ∇²f(xₖ)
   - H[i][j] = ∂²f/(∂xᵢ∂xⱼ)

4. Giải hệ: H × d = -g
   - Tìm hướng Newton d

5. Line search (backtracking):
   - α = 1.0
   - Trong khi f(xₖ + αd) > f(xₖ) - c₁α||g||²:
     * α = α × 0.5

6. Cập nhật: xₖ₊₁ = xₖ + α × d

7. k ← k + 1

Ý tưởng:
- Xấp xỉ f(x) bằng parabol tại xₖ
- Nhảy tới đỉnh parabol
- Hội tụ bậc 2 (quadratic) khi gần nghiệm
   - Tiết kiệm tính toán

2. Gauss-Newton:
   - Đặc biệt cho bài toán bình phương tối thiểu

3. Levenberg-Marquardt:
   - Kết hợp Newton và Gradient Descent
   - H + μI thay vì H

So sánh với Gradient Descent:

| Tính chất        | GD           | Newton       |
|------------------|--------------|---------------|
| Mỗi vòng        | O(n)         | O(n³)        |
| Hội tụ          | Tuyến tính   | Bậc hai       |
| Bộ nhớ          | Thấp         | Cao (ma trận)|
| Tổng vòng lặp  | Nhiều        | Ít            |

Ưu điểm:
- Hội tụ rất nhanh gần cực tiểu
- Không cần điều chỉnh learning rate
- Chính xác cao

Nhược điểm:
- Tốn kém tính toán (O(n³) cho Hessian)
- Tốn bộ nhớ (ma trận n×n)
- Có thể phân kỳ nếu Hessian suy biến
- Không đảm bảo giảm f nếu không dùng line search

Ứng dụng:
- Tối ưu quy mô nhỏ-trung bình
- Logistic Regression
- Neural network (second-order methods)
- Nonlinear least squares
// x_{k+1} = x_k - H^(-1) * gradient
// 
// Uu diem: Hoi tu nhanh (quadratic convergence)
// Nhuoc diem: Tinh toan Hessian dat, can giai he phuong trinh
