THUAT TOAN: Gradient Descent co ban
DO PHUC TAP: O(k * n) voi k la so vong lap, n la chieu cua bien

PSEUDO CODE:
function gradientDescent(x0, gamma, epsilon, maxIter):
    // x0: diem khoi dau
    // gamma: learning rate (buoc nhay)
    // epsilon: sai so hoi tu
    
    x = x0
    
    for iter = 0 to maxIter:
        // Tinh gradient tai x
        grad = gradient_f(x)
        
        // Kiem tra hoi tu
        normGrad = ||grad||
        if normGrad < epsilon:
            break
        
        // Cap nhat bien theo huong nguoc gradient
        x = x - gamma * grad
    
    return x


// Cong thuc cap nhat:
// x_{k+1} = x_k - gamma * gradient_f(x_k)
//
// Gamma qua lon: dao dong, khong hoi tu

MÔ TẢ BẰNG NGÔN NGỮ TỰ NHIÊN:

Thuật toán: Gradient Descent

Input:
• Hàm mục tiêu f(x), khả vi
• Điểm khởi tạo x₀ ∈ ℝⁿ
• Learning rate γ > 0
• Ngưỡng hội tụ ε > 0
• Số vòng lặp tối đa kₘₐₓ

Khởi tạo:
• x = x₀
• k = 0

Lặp:
1. Tính gradient: gₖ = ∇f(xₖ)

2. Kiểm tra hội tụ:
   - Nếu ||gₖ|| < ε hoặc k > kₘₐₓ: dừng

3. Cập nhật: xₖ₊₁ = xₖ - γ × gₖ

4. k = k + 1

Ý tưởng:
- Gradient ∇f(x) chỉ hướng tăng nhanh nhất
- Di chuyển ngược hướng gradient để giảm f(x)

Chọn γ:
- γ nhỏ: hội tụ chậm
- γ lớn: dao động
- Thường dùng: 0.001, 0.01, 0.1
- Logistic Regression
- Tối ưu phi tuyến
// Gamma qua nho: hoi tu cham
